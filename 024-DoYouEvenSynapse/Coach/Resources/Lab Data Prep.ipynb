{
  "metadata": {
    "saveOutput": false,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coach Instructions\n",
        "\n",
        "This Python notebook can be used before the Hack to prepare data files for the participants.  The intent is for the coach to get the latest Open Powerlitfing data from their website (linked in the challenge files).  In order to suport the challenge structure of implementing an inital load followed by incremental loads, it is necessary to split up the full data set.\n",
        "\n",
        "Starting with the full data set, this notebook will extract the most recent meet activity (by date) and save it off to separate files.  These files will simulate new, incremental data for the Hack team's incremental loads.  All remaining historical data will be saved as the initial data load file.\n",
        "\n",
        "The coach can decide how many daily incremental files to create by changing the value of the <b>numDaysToSeparate</b> variable.\n",
        "\n",
        "Coach Data Preparation Steps:\n",
        "\n",
        "1. Create a Synapse Workspace.  Create a Spark pool in the workspace to run this notebook.\n",
        "2. Save the latest Open Powerlifting data file to the root of the workspace storage account.  Put the path to that file in the <b>pathToOpenPowerliftingCSV</b> variable in the first code cell.\n",
        "3. Set the number of daily meet result files to create in the <b>numDaysToSeparate</b> variable in the second code cell.\n",
        "4. Run the notebook.  Daily meet result files (incremental loads) will be created in a subfolder called <b>dailyReports</b> in the workspace storage account.  All remaining historical data will be saved in a single file in a subfolder called <b>initialData</b>.\n",
        "\n",
        "At the coach's discretion, this work can either be done by the coach before the Hack or by the Hack team as part of Challenge 1.\n",
        "\n",
        "### File Format\n",
        "It is possible that the format of the Open Powerlifting data file will change over time.  This notebook is only dependent on the Date and MeetName fields in that file.  Provided those fields are present, with those names, the notebook should still produce the desired results with whatever the current format is.\n",
        "\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "## Replace this variable with full path to the OPL data file in the root container of the Synapse Workspace storage account\n",
        "pathToOpenPowerliftingCSV = \"abfss://<rootContainerName>@<synpaseWorkspaceName>.dfs.core.windows.net/openpowerlifting-2021-01-05.csv\"\n",
        "\n",
        "df = spark.read.csv(pathToOpenPowerliftingCSV, header=True)\n",
        "##display(df.limit(10))\n",
        "df.createOrReplaceTempView(\"LiftingResults\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Date"
            ],
            "values": [
              "Meets"
            ],
            "yLabel": "Meets",
            "xLabel": "Date",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"Meets\":{\"2020-12-17\":2,\"2020-12-18\":3,\"2020-12-19\":16,\"2020-12-20\":3,\"2020-12-26\":1}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "numDaysToSeparate = 5\n",
        "\n",
        "dfMeetsByDate = spark.sql(\"SELECT Date, COUNT(DISTINCT MeetName) AS Meets, COUNT(*) AS Participants FROM LiftingResults GROUP BY Date ORDER BY Date DESC\")\n",
        "listMostRecentDates = dfMeetsByDate.take(numDaysToSeparate)\n",
        "\n",
        "##dfMeetsByDate.show()\n",
        "display(listMostRecentDates)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql.functions import col, asc,desc\n",
        "\n",
        "##Prep target folder path for daily reports\n",
        "if list(filter(lambda x : x.name == \"dailyReports\", mssparkutils.fs.ls(\"/\"))):\n",
        "    mssparkutils.fs.rm(\"/dailyReports\", True)\n",
        "mssparkutils.fs.mkdirs(\"/dailyReports\")\n",
        "\n",
        "##Iterate through the N most recent dates and create a CSV file per date, in the target folder path\n",
        "for row in listMostRecentDates:\n",
        "    activityDate = row[0]\n",
        "    outputFilename = \"daily-results-\" + activityDate + \".csv\"\n",
        "    outputFullPath = \"/\" + outputFilename\n",
        "\n",
        "    df.where(df.Date == activityDate).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"emptyValue\", \"\").csv(outputFullPath)\n",
        "\n",
        "    files = mssparkutils.fs.ls(outputFullPath)\n",
        "    partFilename = list(filter(lambda x : x.name.endswith(\"csv\"), files))\n",
        "    for filename in partFilename:\n",
        "        mssparkutils.fs.mv(filename.path, \"/dailyReports/\" + outputFilename)\n",
        "\n",
        "    mssparkutils.fs.rm(outputFullPath, True)\n",
        "##Done with daily reports\n",
        "\n",
        "##Prep target folder path for initial data.  This is simply the OpenPowerLifting dataset with the N most recent dates removed\n",
        "if list(filter(lambda x : x.name == \"initialData\", mssparkutils.fs.ls(\"/\"))):\n",
        "    mssparkutils.fs.rm(\"/initialData\", True)\n",
        "mssparkutils.fs.mkdirs(\"/initialData\")\n",
        "\n",
        "outputFilename = \"openpowerlifting-initial-data.csv\"\n",
        "outputFullPath = \"/\" + outputFilename\n",
        "\n",
        "##Filter the dataframe to exclude the  N most recent dates and write it out to a single CSV in the \\initalData folder\n",
        "initialDataBeforeThisDate = listMostRecentDates[numDaysToSeparate - 1][0]\n",
        "\n",
        "dfInitialData = df.where(df.Date < initialDataBeforeThisDate).orderBy(col(\"Date\").desc())\n",
        "dfInitialData.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"emptyValue\", \"\").csv(outputFullPath)\n",
        "\n",
        "files = mssparkutils.fs.ls(outputFullPath)\n",
        "partFilename = list(filter(lambda x : x.name.endswith(\"csv\"), files))\n",
        "for filename in partFilename:\n",
        "    mssparkutils.fs.mv(filename.path, \"/initialData/\" + outputFilename)\n",
        "\n",
        "mssparkutils.fs.rm(outputFullPath, True)\n",
        "##Done with initial data"
      ],
      "attachments": {}
    }
  ]
}